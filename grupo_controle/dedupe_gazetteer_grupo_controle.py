#!/usr/bin/python
# -*- coding: utf-8 -*-
"""
This code demonstrates the Gazetteer for Data Matching
between BV-FAPESP and Web of Science datasets

We will use the output files generated by running gera_dados_para_grupo_controle.py
- cj_canonico_pesquisadores.csv as the canonical set.
- cj_messy_autorias_para_treinamento.csv as the messy set for the training process
- cj_messy_autorias_para_validacao.csv as the messy set for the validation process
- cj_messy_autorias_para_teste.csv as the messy set for test the model

Note: If you want to train from scratch, delete the settings_file and the training_file
      saved in [path_to]/grupo_controle/arquivos/dados_treinamento

"""

import os, sys, csv
import pdb
import logging
import optparse
import collections
import dedupe

import abc
from abc import ABC, abstractmethod

from dedupe_gazetteer_utils import (readData,
                                    getTrainingData, getTrueMatchesSet,
                                    getDiceCoefficient, evaluateMatches,
                                    readDataToSaveResults)

from datetime import datetime

################################################################################
### Setup
ARQUIVOS_AUX_DIR = os.path.join(os.path.dirname(__file__),'arquivos/dados_auxiliares')
ARQUIVOS_ENTRADA_DIR = os.path.join(os.path.dirname(__file__),'arquivos/dados_entrada')
ARQUIVOS_TREINAMENTO_DIR = os.path.join(os.path.dirname(__file__),'arquivos/dados_treinamento')
ARQUIVOS_SAIDA_DIR = os.path.join(os.path.dirname(__file__),'arquivos/dados_saida')

# input files
pesquisadores_file = os.path.join(ARQUIVOS_ENTRADA_DIR,'cj_canonico_pesquisadores.csv')
autorias_treinamento_file = os.path.join(ARQUIVOS_ENTRADA_DIR,'cj_messy_autorias_para_treinamento.csv')
autorias_validacao_file = os.path.join(ARQUIVOS_ENTRADA_DIR,'cj_messy_autorias_para_validacao.csv')
autorias_teste_file = os.path.join(ARQUIVOS_ENTRADA_DIR,'cj_messy_autorias_para_teste.csv')

# output files
settings_file = os.path.join(ARQUIVOS_TREINAMENTO_DIR,'gazetteer_learned_settings')
training_file = os.path.join(ARQUIVOS_TREINAMENTO_DIR,'gazetteer_training.json')
output_file = os.path.join(ARQUIVOS_SAIDA_DIR,'gazetteer_matches_found.csv')

false_positives_file = os.path.join(ARQUIVOS_SAIDA_DIR,'gazetteer_false_positives.csv')
false_negatives_file = os.path.join(ARQUIVOS_SAIDA_DIR,'gazetteer_false_negatives.csv')

################################################################################

class TrainingProcess:
    def __init__(self, canonical_set_file, settings_file,training_file):
        self.settings_file = settings_file
        self.training_file = training_file
        self.canonical_file = canonical_set_file
        # self.messy_test_file = ''
        self.canonical_d = readData(canonical_set_file)
        self.messy_test_d = {}
        self.found_matches_s = set()
        self.messy_matches = {}
        self.cluster_membership = {}

    def define_variables(self):
        ### Variables Definition
        # Define the fields the gazetteer will pay attention to, by creating
        # a list of dictionaries describing the variables will be used for training a model.
        # Note that a variable definition describes the records that you want to match, and
        # it is a dictionary where the keys are the fields and the values are the field specification.
        variables = [
                      {'field': 'nome', 'type': 'String'},
                      {'field': 'nome', 'type': 'Text'},
                      {'field': 'primeiro_nome', 'type':'Exact', 'has missing': True},
                      {'field': 'abr', 'type':'ShortString'},
                      {'field': 'ult_sobrenome', 'type': 'Exact'}
                    ]

        return variables

    def training(self, messy_training_set_file, messy_validation_set_file, sample_size = 1000):
        # Reading data
        messy_training_d = readData(messy_training_set_file)
        labeled_pair_groups_list = getTrainingData(messy_d=messy_training_d, canonical_d=self.canonical_d,  sample_size=sample_size)
        messy_validation_d = readData(messy_validation_set_file)
        print(f"Number of records from messy data for validation (autorias): {len(messy_validation_d)}")
        print(f"Number of labeled pair groups: {len(labeled_pair_groups_list)}")

        print("\n[TRAINING PROCESS]")
        # Define the fields the gazetteer will pay attention to
        variables = self.define_variables()

        ### Create a new gazetteer object and pass our data model to it.
        gazetteer = dedupe.Gazetteer(variables)
        tmp_gazetteer = dedupe.Gazetteer(variables)

        i = 0
        stop = False
        n_groups = len(labeled_pair_groups_list)

        while i < n_groups and stop == False:
            print('=================================================================')
            print(f'Iteration: {i}')
            (labeled_messy_d,labeled_canonical_d) = labeled_pair_groups_list[i]

            if i == 0:
                # If we have training data saved from a previous run of gazetteer,
                # look for it an load it in.
                # __Note:__ if you want to train from scratch, delete the training_file
                if os.path.exists(self.training_file):
                    print('Reading labeled examples from ', self.training_file)
                    with open(self.training_file) as tf:
                        gazetteer.prepare_training(labeled_messy_d, labeled_canonical_d, training_file=tf)
                else:
                    gazetteer.prepare_training(labeled_messy_d, labeled_canonical_d, sample_size=150000) #, blocked_proportion=0.5)

                print('Starting Active Labeling...')
                n_distinct_pairs = len(labeled_messy_d)
                labeled_pairs = dedupe.training_data_link(labeled_messy_d, labeled_canonical_d,'link_id',training_size=n_distinct_pairs)
                gazetteer.mark_pairs(labeled_pairs)

                gazetteer.train()

                # When finished, save the training away to disk
                with open(self.training_file, 'w') as tf:
                    gazetteer.write_training(tf)  # Write a JSON file that contains labeled examples (pairs)
                # Computing the Dice coeficient
                dc = getDiceCoefficient(gazetteer_obj=gazetteer, canonical_d=self.canonical_d, validation_d=messy_validation_d)
                print(f'[Validation] Dice Coefient: {round(dc,4)}')

            else:
                print('Reading labeled examples from ', self.training_file)
                with open(self.training_file) as tf:
                    tmp_gazetteer.prepare_training(labeled_messy_d, labeled_canonical_d, training_file=tf)

                print('Starting Active Labeling...')
                n_distinct_pairs = len(labeled_messy_d)
                labeled_pairs = dedupe.training_data_link(labeled_messy_d, labeled_canonical_d,'link_id',training_size=n_distinct_pairs)

                try:
                    tmp_gazetteer.mark_pairs(labeled_pairs)
                    tmp_gazetteer.train()

                    new_dc = getDiceCoefficient(gazetteer_obj=tmp_gazetteer, canonical_d=self.canonical_d, validation_d=messy_validation_d)

                    if new_dc < dc:
                        stop = True
                        print(f'[Validation] Dice Coefient (old): {round(dc,4)}')
                        print(f'[Validation] Dice Coefient (new): {round(new_dc,4)}')
                        print('Stopping...')
                    else:
                        print(f'[Validation] Dice Coefient (new): {round(new_dc,4)}')
                        with open(self.training_file, 'w') as tf:
                            tmp_gazetteer.write_training(tf)  # Write a JSON file that contains labeled examples (pairs)

                        gazetteer.cleanup_training()
                        gazetteer = tmp_gazetteer
                        tmp_gazetteer = dedupe.Gazetteer(variables)
                        dc = new_dc
                except:
                    print("An exception occurred")
                    stop = True
                    pass


            i += 1

        ### Save the weights and predicates to disk.
        with open(self.settings_file, 'wb') as sf:
            gazetteer.write_settings(sf) # Write a settings file containing the data model and predicates.

        ### Clean up data we used for training. Free up memory.
        gazetteer.cleanup_training()
        tmp_gazetteer.cleanup_training()

        print('=================================================================')



class ModelEvaluation():
    """ Model Evaluation """

    # usar classe abstrata com metodos concretos  para o prediction process. IPredict
    # implementar duas classes concretas: PredictForProduction e TestTraining
    # TestForTraining compoem com Model Evaluation e SimularErros

    def __init__(self, canonical_file, messy_test_file, false_positives_file, false_negatives_file):
        self.canonical_file = pesquisadores_file
        self.messy_test_file = autorias_teste_file

        self.false_positives_file = false_positives_file
        self.false_negatives_file = false_negatives_file

        self.canonical_d = None
        self.messy_test_d = None
        self.found_matches_s = None
        self.cluster_membership = None
        self.messy_matches = None


    def true_matches(self):
        n_messy_test = len(self.messy_test_d)
        n_canonical = len(self.canonical_d)

        ### True matches set
        true_matches_s = getTrueMatchesSet(canonical_d=self.canonical_d, messy_d=self.messy_test_d)
        evaluateMatches(found_matches=self.found_matches_s, true_matches=true_matches_s, n_messy_test=n_messy_test, n_canonical=n_canonical)

        true_positives_s = self.found_matches_s.intersection(true_matches_s)
        self.false_positives_s = self.found_matches_s.difference(true_positives_s)
        self.false_negatives_s = true_matches_s.difference(self.found_matches_s)



    def save_false_positives(self):
        ### salvando informacao obtida da clusterizacao

        canon_file = self.canonical_file
        messy_file = self.messy_test_file

        canonical_d = readDataToSaveResults(canon_file) #self.canonical_d
        messy_test_d = readDataToSaveResults(messy_file) #self.messy_test_d

        messy_matches = self.messy_matches
        cluster_membership = self.cluster_membership


        false_positives_d = collections.defaultdict(list)
        for (record_id_1, record_id_2) in self.false_positives_s:
            canon_record_id = record_id_1 if canon_file in record_id_1 else record_id_2
            if canon_record_id not in false_positives_d.keys():
                false_positives_d[canon_record_id] = []

        for (record_id_1, record_id_2) in self.false_positives_s:
            messy_record_id = record_id_1 if messy_file in record_id_1 else record_id_2
            canon_record_id = record_id_1 if canon_file in record_id_1 else record_id_2
            false_positives_d[canon_record_id].append(messy_record_id)


        # obtendo cabeçalho original
        with open(canon_file, newline='') as csvfile:
            reader = csv.DictReader(csvfile, delimiter='|')
            headers = reader.fieldnames


        # salvando falsos positivos
        with open(self.false_positives_file, 'w') as f_output:
            fieldnames = ['cluster_id','link_score','source_file', 'record_id'] + headers
            writer = csv.DictWriter(f_output, delimiter='|', fieldnames=fieldnames)
            writer.writeheader() # save the new fieldnames

            # Salvando:
            # - entidades do conjunto canonico, cuja correspondencia foi encontrada incorretamente
            # - entidades do conjunto messy, cuja correspondencia foi encontrada incorretamente
            for canon_record_id, messy_record_ids in list(false_positives_d.items()):
                cluster_id =  cluster_membership[canon_record_id]
                canon_record_row = canonical_d[canon_record_id].copy()
                cluster_details = {'cluster_id':cluster_id, 'link_score':None, 'source_file':canon_file, 'record_id':canon_record_id}
                canon_record_row.update(cluster_details)
                writer.writerow(canon_record_row)
                for messy_record_id in messy_record_ids:
                    score = messy_matches[messy_record_id][canon_record_id]
                    messy_record_row = messy_test_d[messy_record_id].copy()
                    cluster_details = {'cluster_id':cluster_id, 'link_score':score, 'source_file':messy_file, 'record_id':messy_record_id}
                    messy_record_row.update(cluster_details)
                    writer.writerow(messy_record_row)

    def save_false_negatives(self):
        ### salvando informacao obtida da clusterizacao

        canon_file = self.canonical_file
        messy_file = self.messy_test_file

        canonical_d = readDataToSaveResults(canon_file) #self.canonical_d
        messy_test_d = readDataToSaveResults(messy_file) #self.messy_test_d


        false_negatives_d = collections.defaultdict(list)
        for (record_id_1, record_id_2) in self.false_negatives_s:
            canon_record_id = record_id_1 if canon_file in record_id_1 else record_id_2
            if canon_record_id not in false_negatives_d.keys():
                false_negatives_d[canon_record_id] = []

        for (record_id_1, record_id_2) in self.false_negatives_s:
            messy_record_id = record_id_1 if messy_file in record_id_1 else record_id_2
            canon_record_id = record_id_1 if canon_file in record_id_1 else record_id_2
            false_negatives_d[canon_record_id].append(messy_record_id)

        # obtendo cabeçalho original
        with open(canon_file, newline='') as csvfile:
            reader = csv.DictReader(csvfile, delimiter='|')
            headers = reader.fieldnames

        # salvando falsos negativos
        with open(self.false_negatives_file, 'w') as f_output:
            fieldnames = ['cluster_id','link_score','source_file', 'record_id'] + headers
            writer = csv.DictWriter(f_output, delimiter='|', fieldnames=fieldnames)
            writer.writeheader() # save the new fieldnames

            # Salvando:
            # - entidades do conjunto canonico, cuja correspondencia nao foi encontrada
            # - entidades do conjunto messy, cuja correspondencia nao foi encontrada
            for canon_record_id, messy_record_ids in list(false_negatives_d.items()):
                canon_record_row = canonical_d[canon_record_id].copy()
                cluster_details = {'cluster_id':None, 'link_score':None,
                                   'source_file':canon_file, 'record_id':canon_record_id}
                canon_record_row.update(cluster_details)
                writer.writerow(canon_record_row)
                for messy_record_id in messy_record_ids:
                    messy_record_row = messy_test_d[messy_record_id].copy()
                    cluster_details = { 'cluster_id':None, 'link_score':None,
                                        'source_file': messy_file,'record_id': messy_record_id}
                    messy_record_row.update(cluster_details)
                    writer.writerow(messy_record_row)


class IPredict(ABC):
    @abstractmethod
    def __init__():
        raise NotImplementedError()

    def cluster_data(self):
        print('Clustering new data...')
        print('Reading from', self.settings_file)
        with open(settings_file, 'rb') as sf:
            gazetteer = dedupe.StaticGazetteer(sf)
        gazetteer.index(self.canonical_d)

        results = gazetteer.search(self.messy_test_d, threshold=0.5, n_matches=1, generator=True)
        # n_matches=1 -> se obtem um unico pesquisador associado por cada autor de artigo em messy_test_d
        # se u valor nao eh dado, a busca retorna todos os possiveis matches acima do threshold
        # generator=True -> o match gera uma sequencia de possiveis matches, em vez de uma lista.
        # Default generator=False.

        messy_matches = collections.defaultdict(dict) # dicionario cujos valores sao dicionarios
        found_matches_s = set()

        # format of an item in "results": (messy_record_id,((canon_record_id,score),...))
        for messy_record_id, matches in results:
            for canon_record_id, score in matches: # matches = ((canon_record_id,score),...)
                messy_matches[messy_record_id][canon_record_id] = score
                found_pair = (messy_record_id, canon_record_id)
                found_matches_s.add(frozenset(found_pair))


        cluster_membership = {}
        cluster_id = 0

        for messy_record_id in messy_matches.keys():
            matches = messy_matches[messy_record_id] # dictionary with a single element for n_matches=1
            for canon_record_id in matches:
                if canon_record_id not in cluster_membership:
                    cluster_membership[canon_record_id] = cluster_id
                    cluster_id += 1

        self.found_matches_s = found_matches_s
        self.messy_matches = messy_matches
        self.cluster_membership = cluster_membership
        self.save_prediction_result(self.output_file)



class ProductionPredict(IPredict):
    """ Classication or Bloking Process """

    def __init__(self, canonical_set_file, settings_file, messy_test_set_file):
        print('\n[PREDICTION PROCESS]')
        self.messy_test_file = messy_test_set_file
        self.output_file = output_file

        print('Importing messy test data ...')
        self.messy_test_d = readData(messy_test_set_file)
        print(f"Number of records from messy data for test (autorias): {len(self.messy_test_d)}")

        print('Reading from', settings_file)
        self.settings_file = settings_file

        self.canonical_file = canonical_set_file
        self.canonical_d = readData(canonical_set_file)


class TrainingTest(IPredict):
    def __init__(self, canonical_set_file, settings_file, messy_test_set_file, output_file, model_evaluation:ModelEvaluation):
        print('\n[PREDICTION PROCESS]')
        self.messy_test_file = messy_test_set_file
        self.output_file = output_file

        print('Importing messy test data ...')
        self.messy_test_d = readData(messy_test_set_file)
        print(f"Number of records from messy data for test (autorias): {len(self.messy_test_d)}")

        print('Reading from', settings_file)
        self.settings_file = settings_file

        self.canonical_file = canonical_set_file
        self.canonical_d = readData(canonical_set_file)

        self.model_evaluation = model_evaluation


    def save_prediction_result(self, output_file):
        """ salvando informacao obtida da clusterizacao """
        canon_file = self.canonical_file
        messy_file = self.messy_test_file

        canonical_d = readDataToSaveResults(canon_file) #self.canonical_d
        messy_test_d = readDataToSaveResults(messy_file) #self.messy_test_d

        found_matches_s = self.found_matches_s
        messy_matches = self.messy_matches
        cluster_membership = self.cluster_membership


        found_matches_d = collections.defaultdict(list)
        for (record_id_1, record_id_2) in found_matches_s:
            canon_record_id = record_id_1 if canon_file in record_id_1 else record_id_2
            if canon_record_id not in found_matches_d.keys():
                found_matches_d[canon_record_id] = []

        for (record_id_1, record_id_2) in found_matches_s:
            messy_record_id = record_id_1 if messy_file in record_id_1 else record_id_2
            canon_record_id = record_id_1 if canon_file in record_id_1 else record_id_2
            found_matches_d[canon_record_id].append(messy_record_id)

        # obtendo cabeçalho original
        with open(canon_file, newline='') as csvfile:
            reader = csv.DictReader(csvfile, delimiter='|')
            headers = reader.fieldnames

        # salvando resultados
        with open(output_file, 'w') as f_output:
            fieldnames = ['cluster_id','link_score','source_file', 'record_id'] + headers
            writer = csv.DictWriter(f_output, delimiter='|', fieldnames=fieldnames)
            writer.writeheader() # save the new fieldnames

            # Salvando:
            # - entidades encontradas do conjunto canonico
            # - entidades encontradas do conjunto messy
            for canon_record_id, messy_record_ids in list(found_matches_d.items()):
                cluster_id =  cluster_membership[canon_record_id]
                canon_record_row = canonical_d[canon_record_id].copy()
                cluster_details = {'cluster_id':cluster_id, 'link_score':None, 'source_file':canon_file, 'record_id':canon_record_id}
                canon_record_row.update(cluster_details)
                writer.writerow(canon_record_row)
                for messy_record_id in messy_record_ids:
                    score = messy_matches[messy_record_id][canon_record_id]
                    messy_record_row = messy_test_d[messy_record_id].copy()
                    cluster_details = {'cluster_id':cluster_id, 'link_score':score, 'source_file':messy_file, 'record_id':messy_record_id}
                    messy_record_row.update(cluster_details)
                    writer.writerow(messy_record_row)

    def evaluate_model(self):
        self.model_evaluation.canonical_d = self.canonical_d
        self.model_evaluation.messy_test_d = self.messy_test_d
        self.model_evaluation.found_matches_s = self.found_matches_s
        self.model_evaluation.cluster_membership = self.cluster_membership
        self.model_evaluation.messy_matches = self.messy_matches

        self.model_evaluation.true_matches()
        self.model_evaluation.save_false_positives()
        self.model_evaluation.save_false_negatives()


if __name__ == '__main__':

    ### Logging

    # Dedupe uses Python logging to show or suppress verbose output.
    # Added for convenience.
    # To enable verbose logging, run:
    # python [path_to]/dedupe_gazetteer_grupo_controle.py -v
    optp = optparse.OptionParser()
    optp.add_option('-v', '--verbose', dest='verbose', action='count',
                    help='Increase verbosity (specify multiple times for more)'
                    )
    (opts, args) = optp.parse_args()
    log_level = logging.WARNING
    if opts.verbose:
        if opts.verbose == 1:
            log_level = logging.INFO
        elif opts.verbose >= 2:
            log_level = logging.DEBUG
    logging.getLogger().setLevel(log_level)


    start_time = datetime.now()
    step_time = 0
    end_time = 0

    """
    ### Training
    tp = TrainingProcess(pesquisadores_file, settings_file, training_file)
    print(f"Number of records from canonical data (pesquisadores unicos): {len(tp.canonical_d)}")

    # __Note: If you want to train from scratch, delete the settings_file and the training_file
    if not os.path.exists(settings_file):
        ### Training
        # sample_size: number of positive matches to be considered for each iteration in the training process
        sample_size = 1000
        step_time = datetime.now()
        tp.training(autorias_treinamento_file, autorias_validacao_file, sample_size = 1000)
        end_time = datetime.now()
        print(f"Training Time: {end_time - step_time} \n")
    """

    ### Prediction
    step_time = datetime.now()
    model_evaluation = ModelEvaluation(pesquisadores_file, autorias_teste_file, false_positives_file, false_negatives_file)
    tt = TrainingTest(pesquisadores_file, settings_file, autorias_teste_file, output_file, model_evaluation)
    tt.cluster_data()
    end_time = datetime.now()
    print(f"Prediction Time and File Recording Time: {end_time - step_time} \n")
    ###

    ### Performance of the algorithm
    step_time = datetime.now()
    tt.evaluate_model()
    end_time = datetime.now()
    print(f"Model Evaluation Time and Files Recording Time: {end_time - step_time} \n")
    ###

    print(f'Total Processing Time: {end_time - start_time}')