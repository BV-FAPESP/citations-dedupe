#!/usr/bin/python
# -*- coding: utf-8 -*-
"""
This code demonstrates the Gazetteer for Data Matching
between BV-FAPESP and Web of Science datasets

We will use the output files generated by running gera_dados_para_grupo_controle.py
- cj_canonico_pesquisadores.csv as the canonical set.
- cj_messy_autorias_para_treinamento.csv as the messy set for the training process
- cj_messy_autorias_para_validacao.csv as the messy set for the validation process
- cj_messy_autorias_para_teste.csv as the messy set for test the model

Note: If you want to train from scratch, delete the settings_file and the training_file
      saved in [path_to]/grupo_controle/arquivos/dados_treinamento

"""

import os, sys, csv, copy
from random import sample
import pdb
import logging
import optparse
import collections
import dedupe
import string
import random
import math

from ..generic_utils import *

import abc
from abc import ABC, abstractmethod

from grupo_controle.dedupe_gazetteer_utils import (readData,
                                    getTrainingData, getTrueMatchesSet,
                                    getDiceCoefficient, evaluateMatches,
                                    readDataToSaveResults, chk_file_exists)

from datetime import datetime

################################################################################
### Setup
ARQUIVOS_AUX_DIR = os.path.join(os.path.dirname(__file__),'arquivos/dados_auxiliares')
ARQUIVOS_ENTRADA_DIR = os.path.join(os.path.dirname(__file__),'arquivos/dados_entrada')
ARQUIVOS_TREINAMENTO_DIR = os.path.join(os.path.dirname(__file__),'arquivos/dados_treinamento')
ARQUIVOS_SAIDA_DIR = os.path.join(os.path.dirname(__file__),'arquivos/dados_saida')

# input files
ip_canonical_file = os.path.join(ARQUIVOS_ENTRADA_DIR,'cj_canonico_pesquisadores.csv')
ip_messy_training_file = os.path.join(ARQUIVOS_ENTRADA_DIR,'cj_messy_autorias_para_treinamento.csv')
ip_messy_validation_file = os.path.join(ARQUIVOS_ENTRADA_DIR,'cj_messy_autorias_para_validacao.csv')
ip_messy_test_file = os.path.join(ARQUIVOS_ENTRADA_DIR,'cj_messy_autorias_para_teste.csv')

# output files
op_settings_file = os.path.join(ARQUIVOS_TREINAMENTO_DIR,'gazetteer_learned_settings')
op_training_file = os.path.join(ARQUIVOS_TREINAMENTO_DIR,'gazetteer_training.json')
op_matches_found_file = os.path.join(ARQUIVOS_SAIDA_DIR,'gazetteer_matches_found.csv')
op_false_positives_file = os.path.join(ARQUIVOS_SAIDA_DIR,'gazetteer_false_positives.csv')
op_false_negatives_file = os.path.join(ARQUIVOS_SAIDA_DIR,'gazetteer_false_negatives.csv')

################################################################################


class TrainingElement:

    ### Variables Definition
    # Define the fields the gazetteer will pay attention to, by creating
    # a list of dictionaries describing the variables will be used for training a model.
    # Note that a variable definition describes the records that you want to match, and
    # it is a dictionary where the keys are the fields and the values are the field specification.
    VARIABLES = [
                    {'field': 'nome', 'type': 'String'},
                    {'field': 'nome', 'type': 'Text'},
                    {'field': 'primeiro_nome', 'type':'Exact', 'has missing': True},
                    {'field': 'abr', 'type':'ShortString'},
                    {'field': 'ult_sobrenome', 'type': 'Exact'}
                ]

    def __init__(self, op_training_file):
        self.gazetteer = dedupe.Gazetteer(self.VARIABLES)
        self.op_training_file = op_training_file
        self.performance = None

    def prepare_training(self, labeled_messy_d, labeled_canonical_d, op_training_file=None, sample_size=None):
        """ Pre training required """
        self.labeled_messy_d = labeled_messy_d
        self.labeled_canonical_d = labeled_canonical_d
        if op_training_file:
            self.gazetteer.prepare_training(self.labeled_messy_d, self.labeled_canonical_d, training_file=op_training_file)
        elif sample_size:
            self.gazetteer.prepare_training(self.labeled_messy_d, self.labeled_canonical_d, sample_size=sample_size)

    def active_labeling(self):
        """ Mark pairs for already labeled data and train """
        print('Starting Active Labeling...')
        n_distinct_pairs = len(self.labeled_messy_d)
        labeled_pairs = dedupe.training_data_link(self.labeled_messy_d, self.labeled_canonical_d, 'link_id', training_size=n_distinct_pairs)
        self.gazetteer.mark_pairs(labeled_pairs)
        self.gazetteer.train()

    def write_training(self):
        """ When finished, save the training away to disk
        Write a JSON file that contains labeled examples (pairs) """
        with open(self.op_training_file, 'w') as tf:
            self.gazetteer.write_training(tf)

    def training_performance(self, value):
        self.performance = value

    def __ge__(self, other):
        return self.performance >= other.performance

    def __str__(self):
        return str(round(self.performance, 4))





class TrainingProcess:
    def __init__(self, ip_canonical_file, op_settings_file, op_training_file):
        self.op_settings_file = op_settings_file
        self.op_training_file = op_training_file
        self.canonical_d = readData(ip_canonical_file)

        # Always remove settings and training files on __init__
        try:
            os.unlink(self.op_settings_file)
            os.unlink(self.op_training_file)
        except FileNotFoundError:
            pass

    def training(self, ip_messy_training_file, ip_messy_validation_file, sample_size = 1000):
        # Reading data
        messy_training_d = readData(ip_messy_training_file)
        messy_validation_d = readData(ip_messy_validation_file)
        labeled_pair_groups_list = getTrainingData(messy_d=messy_training_d, canonical_d=self.canonical_d,  sample_size=sample_size)
        print(f"Number of records from messy data for validation (autorias): {len(messy_validation_d)}")
        print(f"Number of labeled pair groups: {len(labeled_pair_groups_list)}")
        print("\n[TRAINING PROCESS]")
        trained_element = TrainingElement(self.op_training_file)

        i = 0
        stop = False
        n_groups = len(labeled_pair_groups_list)

        while i < n_groups and stop == False:
            print('=================================================================')
            print(f'Iteration: {i}')
            (labeled_messy_d, labeled_canonical_d) = labeled_pair_groups_list[i]
            if i == 0:
                trained_element.prepare_training(labeled_messy_d, labeled_canonical_d, sample_size=150000)
                trained_element.active_labeling()
                trained_element.write_training()

                dc = getDiceCoefficient(gazetteer_obj=trained_element.gazetteer, canonical_d=self.canonical_d, validation_d=messy_validation_d)
                trained_element.training_performance(dc)
                print(f'[Validation] Dice Coefient: {trained_element}')
            else:
                try:
                    with open(self.op_training_file) as tf:
                        print('Reading labeled examples from ', self.op_training_file)
                        trained_element.prepare_training(labeled_messy_d, labeled_canonical_d, op_training_file=tf)
                        trained_element.active_labeling()
                except IOError:
                    raise

                try:
                    dc = getDiceCoefficient(gazetteer_obj=trained_element.gazetteer, canonical_d=self.canonical_d, validation_d=messy_validation_d)
                    trained_element.training_performance(dc)

                    if trained_element >= former_trained_element:
                        print(f'[Validation] Dice Coefient (new): {trained_element}')
                        trained_element.write_training()
                    else:
                        stop = True
                        print(f'[Validation] Dice Coefient (old): {former_trained_element}')
                        print(f'[Validation] Dice Coefient (new): {trained_element}')
                        print('Stopping...')
                except Exception as e:
                    raise

            i += 1
            former_trained_element = copy.deepcopy(trained_element)

        ### Save the weights and predicates to disk.
        with open(self.op_settings_file, 'wb') as sf:
            trained_element.gazetteer.write_settings(sf) # Write a settings file containing the data model and predicates.

        ### Clean up data we used for training. Free up memory.
        trained_element.gazetteer.cleanup_training()

        print('=================================================================')



class ModelEvaluation():
    """ Model Evaluation """

    def __init__(self, ip_canonical_file, ip_messy_test_file, op_false_positives_file, op_false_negatives_file):
        self.ip_canonical_file = ip_canonical_file
        self.ip_messy_test_file = ip_messy_test_file
        self.op_false_positives_file = op_false_positives_file
        self.op_false_negatives_file = op_false_negatives_file
        self.canonical_2_save_d = readDataToSaveResults(self.ip_canonical_file)
        self.messy_test_2_save_d = readDataToSaveResults(self.ip_messy_test_file)


    def true_matches(self, canonical_d, messy_test_d, found_matches_s):
        n_messy_test = len(messy_test_d)
        n_canonical = len(canonical_d)

        ### True matches set
        true_matches_s = getTrueMatchesSet(canonical_d=canonical_d, messy_d=messy_test_d)
        evaluateMatches(found_matches=found_matches_s, true_matches=true_matches_s, n_messy_test=n_messy_test, n_canonical=n_canonical)

        true_positives_s = found_matches_s.intersection(true_matches_s)
        self.false_positives_s = found_matches_s.difference(true_positives_s)
        self.false_negatives_s = true_matches_s.difference(found_matches_s)



    def save_false_positives(self, messy_matches, cluster_membership):
        ### salvando informacao obtida da clusterizacao

        false_positives_d = collections.defaultdict(list)
        for (record_id_1, record_id_2) in self.false_positives_s:
            canon_record_id = record_id_1 if self.ip_canonical_file in record_id_1 else record_id_2
            if canon_record_id not in false_positives_d.keys():
                false_positives_d[canon_record_id] = []

        for (record_id_1, record_id_2) in self.false_positives_s:
            messy_record_id = record_id_1 if self.ip_messy_test_file in record_id_1 else record_id_2
            canon_record_id = record_id_1 if self.ip_canonical_file in record_id_1 else record_id_2
            false_positives_d[canon_record_id].append(messy_record_id)

        # obtendo cabeçalho original
        with open(self.ip_canonical_file, newline='') as csvfile:
            reader = csv.DictReader(csvfile, delimiter='|')
            headers = reader.fieldnames

        # salvando falsos positivos
        with open(self.op_false_positives_file, 'w') as f_output:
            fieldnames = ['cluster_id','link_score','source_file', 'record_id'] + headers
            writer = csv.DictWriter(f_output, delimiter='|', fieldnames=fieldnames)
            writer.writeheader() # save the new fieldnames

            # Salvando:
            # - entidades do conjunto canonico, cuja correspondencia foi encontrada incorretamente
            # - entidades do conjunto messy, cuja correspondencia foi encontrada incorretamente
            for canon_record_id, messy_record_ids in list(false_positives_d.items()):
                cluster_id =  cluster_membership[canon_record_id]
                canon_record_row = self.canonical_2_save_d[canon_record_id].copy()
                cluster_details = {'cluster_id':cluster_id, 'link_score':None, 'source_file':self.ip_canonical_file, 'record_id':canon_record_id}
                canon_record_row.update(cluster_details)
                writer.writerow(canon_record_row)
                for messy_record_id in messy_record_ids:
                    score = messy_matches[messy_record_id][canon_record_id]
                    messy_record_row = self.messy_test_2_save_d[messy_record_id].copy()
                    cluster_details = {'cluster_id':cluster_id, 'link_score':score, 'source_file':self.ip_messy_test_file, 'record_id':messy_record_id}
                    messy_record_row.update(cluster_details)
                    writer.writerow(messy_record_row)

    def save_false_negatives(self):
        ### salvando informacao obtida da clusterizacao

        false_negatives_d = collections.defaultdict(list)
        for (record_id_1, record_id_2) in self.false_negatives_s:
            canon_record_id = record_id_1 if self.ip_canonical_file in record_id_1 else record_id_2
            if canon_record_id not in false_negatives_d.keys():
                false_negatives_d[canon_record_id] = []

        for (record_id_1, record_id_2) in self.false_negatives_s:
            messy_record_id = record_id_1 if self.ip_messy_test_file in record_id_1 else record_id_2
            canon_record_id = record_id_1 if self.ip_canonical_file in record_id_1 else record_id_2
            false_negatives_d[canon_record_id].append(messy_record_id)

        # obtendo cabeçalho original
        with open(self.ip_canonical_file, newline='') as csvfile:
            reader = csv.DictReader(csvfile, delimiter='|')
            headers = reader.fieldnames

        # salvando falsos negativos
        with open(self.op_false_negatives_file, 'w') as f_output:
            fieldnames = ['cluster_id','link_score','source_file', 'record_id'] + headers
            writer = csv.DictWriter(f_output, delimiter='|', fieldnames=fieldnames)
            writer.writeheader() # save the new fieldnames

            # Salvando:
            # - entidades do conjunto canonico, cuja correspondencia nao foi encontrada
            # - entidades do conjunto messy, cuja correspondencia nao foi encontrada
            for canon_record_id, messy_record_ids in list(false_negatives_d.items()):
                canon_record_row = self.canonical_2_save_d[canon_record_id].copy()
                cluster_details = {'cluster_id':None, 'link_score':None,
                                   'source_file':self.ip_canonical_file, 'record_id':canon_record_id}
                canon_record_row.update(cluster_details)
                writer.writerow(canon_record_row)
                for messy_record_id in messy_record_ids:
                    messy_record_row = self.messy_test_2_save_d[messy_record_id].copy()
                    cluster_details = { 'cluster_id':None, 'link_score':None,
                                        'source_file': self.ip_messy_test_file,'record_id': messy_record_id}
                    messy_record_row.update(cluster_details)
                    writer.writerow(messy_record_row)


class IPredict(ABC):
    @abstractmethod
    def __init__():
        raise NotImplementedError()

    def cluster_data(self):
        print('Clustering new data...')
        print('Reading from', self.op_settings_file)
        with open(op_settings_file, 'rb') as sf:
            gazetteer = dedupe.StaticGazetteer(sf)
        gazetteer.index(self.canonical_d)

        results = gazetteer.search(self.messy_test_d, threshold=0.5, n_matches=1, generator=True)
        # n_matches=1 -> se obtem um unico pesquisador associado por cada autor de artigo em messy_test_d
        # se u valor nao eh dado, a busca retorna todos os possiveis matches acima do threshold
        # generator=True -> o match gera uma sequencia de possiveis matches, em vez de uma lista.
        # Default generator=False.

        messy_matches = collections.defaultdict(dict) # dicionario cujos valores sao dicionarios
        found_matches_s = set()

        # format of an item in "results": (messy_record_id,((canon_record_id,score),...))
        for messy_record_id, matches in results:
            for canon_record_id, score in matches: # matches = ((canon_record_id,score),...)
                messy_matches[messy_record_id][canon_record_id] = score
                found_pair = (messy_record_id, canon_record_id)
                found_matches_s.add(frozenset(found_pair))


        cluster_membership = {}
        cluster_id = 0

        for messy_record_id in messy_matches.keys():
            matches = messy_matches[messy_record_id] # dictionary with a single element for n_matches=1
            for canon_record_id in matches:
                if canon_record_id not in cluster_membership:
                    cluster_membership[canon_record_id] = cluster_id
                    cluster_id += 1

        self.found_matches_s = found_matches_s
        self.messy_matches = messy_matches
        self.cluster_membership = cluster_membership
        self.save_prediction_result()



class ProductionPredict(IPredict):
    """ Classication or Bloking Process """

    def __init__(self, ip_canonical_file, ip_messy_test_file, op_settings_file, op_matches_found_file):
        print('\n[PREDICTION PROCESS]')
        self.ip_messy_test_file = ip_messy_test_file
        self.op_matches_found_file = op_matches_found_file

        print('Importing messy test data ...')
        self.messy_test_d = readData(ip_messy_test_file)
        print(f"Number of records from messy data for test (autorias): {len(self.messy_test_d)}")

        print('Reading from', op_settings_file)
        self.op_settings_file = op_settings_file

        self.ip_canonical_file = ip_canonical_file
        self.canonical_d = readData(ip_canonical_file)

class Noisify():
    """
        Generates data augmentation adding noise based on percentual or number of characters.
    """
    def __init__(self, data, amount, percentual=True):
        self.data = data
        self.amount = amount
        self.percentual = percentual # if not true, amount is treated as number of characters

    def add_noise(self, name):
        if self.percentual:
            number_of_characters = math.ceil(((len(name)*self.amount/100)))
        else:
            number_of_characters = self.amount
        special_characters_indices = [i for i, c in enumerate(name) 
                                      if c == ' ' or c == '.' or c == ',']
        noise_indices = []
        for i in range(number_of_characters):
            noise_indices.append(random.choice([i for i in range(0,len(name)) 
                                                if i not in special_characters_indices]))
        s = list(name)
        for noise in noise_indices:
            s[noise] = random.choice(string.ascii_letters)
        
        return ''.join(s)

    def get_noisy_data(self):
        self.data['nome'] = self.data['nome'].apply(lambda x: add_noise(x))
        self.data['primeiro_nome'] = self.data['nome'].apply(lambda x: getLongFirstName(remover_acentos(x)))
        self.data['abr'] = self.data['nome'].apply(lambda x: getPartialAbbreviation(remover_acentos(x)))
        self.data['ult_sobrenome'] = self.data['nome'].apply(lambda x: getLastName(remover_acentos(x)))
        
        return self.data


class TrainingTest(IPredict):
    def __init__(self, ip_canonical_file, ip_messy_test_file, op_settings_file, op_matches_found_file, model_evaluation:ModelEvaluation):
        print('\n[PREDICTION PROCESS]')
        self.ip_messy_test_file = ip_messy_test_file
        self.op_matches_found_file = op_matches_found_file

        print('Importing messy test data ...')
        self.messy_test_d = readData(ip_messy_test_file)
        print(f"Number of records from messy data for test (autorias): {len(self.messy_test_d)}")

        print('Reading from', op_settings_file)
        self.op_settings_file = op_settings_file

        self.ip_canonical_file = ip_canonical_file
        self.canonical_d = readData(ip_canonical_file)

        self.model_evaluation = model_evaluation


    def save_prediction_result(self):
        """ salvando informacao obtida da clusterizacao """

        canonical_d = readDataToSaveResults(self.ip_canonical_file) #self.canonical_d
        messy_test_d = readDataToSaveResults(self.ip_messy_test_file) #self.messy_test_d

        found_matches_s = self.found_matches_s
        messy_matches = self.messy_matches
        cluster_membership = self.cluster_membership


        found_matches_d = collections.defaultdict(list)
        for (record_id_1, record_id_2) in found_matches_s:
            canon_record_id = record_id_1 if self.ip_canonical_file in record_id_1 else record_id_2
            if canon_record_id not in found_matches_d.keys():
                found_matches_d[canon_record_id] = []

        for (record_id_1, record_id_2) in found_matches_s:
            messy_record_id = record_id_1 if self.ip_messy_test_file in record_id_1 else record_id_2
            canon_record_id = record_id_1 if self.ip_canonical_file in record_id_1 else record_id_2
            found_matches_d[canon_record_id].append(messy_record_id)

        # obtendo cabeçalho original
        with open(self.ip_canonical_file, newline='') as csvfile:
            reader = csv.DictReader(csvfile, delimiter='|')
            headers = reader.fieldnames

        # salvando resultados
        with open(self.op_matches_found_file, 'w') as f_output:
            fieldnames = ['cluster_id','link_score','source_file', 'record_id'] + headers
            writer = csv.DictWriter(f_output, delimiter='|', fieldnames=fieldnames)
            writer.writeheader() # save the new fieldnames

            # Salvando:
            # - entidades encontradas do conjunto canonico
            # - entidades encontradas do conjunto messy
            for canon_record_id, messy_record_ids in list(found_matches_d.items()):
                cluster_id =  cluster_membership[canon_record_id]
                canon_record_row = canonical_d[canon_record_id].copy()
                cluster_details = {'cluster_id':cluster_id, 'link_score':None, 'source_file':self.ip_canonical_file, 'record_id':canon_record_id}
                canon_record_row.update(cluster_details)
                writer.writerow(canon_record_row)
                for messy_record_id in messy_record_ids:
                    score = messy_matches[messy_record_id][canon_record_id]
                    messy_record_row = messy_test_d[messy_record_id].copy()
                    cluster_details = {'cluster_id':cluster_id, 'link_score':score, 'source_file':self.ip_messy_test_file, 'record_id':messy_record_id}
                    messy_record_row.update(cluster_details)
                    writer.writerow(messy_record_row)

    def evaluate_model(self):
        self.model_evaluation.true_matches(self.canonical_d, self.messy_test_d, self.found_matches_s)
        self.model_evaluation.save_false_positives(self.messy_matches, self.cluster_membership)
        self.model_evaluation.save_false_negatives()

    def noisify(self, amount, percentual):
        noisify = Noisify(self.messy_test_d, amount, percentual)
        noisy_data = noisify.get_noisy_data()
        return noisy_data


if __name__ == '__main__':

    ### Logging

    # Dedupe uses Python logging to show or suppress verbose output.
    # Added for convenience.
    # To enable verbose logging, run:
    # python [path_to]/dedupe_gazetteer_grupo_controle.py -v
    optp = optparse.OptionParser()
    optp.add_option('-v', '--verbose', dest='verbose', action='count',
                    help='Increase verbosity (specify multiple times for more)'
                    )
    (opts, args) = optp.parse_args()
    log_level = logging.WARNING
    if opts.verbose:
        if opts.verbose == 1:
            log_level = logging.INFO
        elif opts.verbose >= 2:
            log_level = logging.DEBUG
    logging.getLogger().setLevel(log_level)


    start_time = datetime.now()
    step_time = 0
    end_time = 0


    ### Training
    """
    Skipt training with you have already trained otherwhise it will delete your trained and settings file,
    and train again.
    """
    tp = TrainingProcess(ip_canonical_file, op_settings_file, op_training_file)
    print(f"Number of records from canonical data (pesquisadores unicos): {len(tp.canonical_d)}")

    sample_size = 1000
    step_time = datetime.now()
    tp.training(ip_messy_training_file, ip_messy_validation_file, sample_size = 1000)
    end_time = datetime.now()
    print(f"Training Time: {end_time - step_time} \n")
    ###


    ### Prediction
    step_time = datetime.now()
    model_evaluation = ModelEvaluation(ip_canonical_file, ip_messy_test_file, op_false_positives_file, op_false_negatives_file)
    tt = TrainingTest(ip_canonical_file, ip_messy_test_file, op_settings_file, op_matches_found_file, model_evaluation)
    tt.cluster_data()
    end_time = datetime.now()
    print(f"Prediction Time and File Recording Time: {end_time - step_time} \n")
    ###

    ### Performance of the algorithm
    step_time = datetime.now()
    tt.evaluate_model()
    end_time = datetime.now()
    print(f"Model Evaluation Time and Files Recording Time: {end_time - step_time} \n")
    ###

    # print(f'Total Processing Time: {end_time - start_time}')
